{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning to Decode 7T-like MR Image Reconstruction from 3T MR Images:   Part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, youâ€™ll implement a medical imaging using deep learning paper with Python in Keras. You will be working on image super-resolution problem using a novel deep learning architecture. The task at hand would be to do a non-linear mapping from a low-field 3-Tesla Brain MR Image to a high-field 7-Tesla Brain MR Image. <br>\n",
    "<br>\n",
    "\n",
    "The tutorial is divided in two parts, this part will walk you through the training process and the second tutorial will cover the testing part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: You would probably be interested in reading the paper, if so, you can find the paper <a href=\" https://arxiv.org/abs/1806.06886\" target=\"_blank\">here</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a nutshell, you'll address the following topics in today's tutorial:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul>\n",
    "    <li>You will start off by importing the required <a href='#modules'>modules</a> that you will need in order to train your deep learning model,</li>\n",
    "<li>Then you will be briefed about <a href='#data'>3T and 7T</a> MRI dataset,\n",
    "<li>Then you'll define the <a href='#initialisers'>intializers</a> and <a href='#load'>load</a> the 3T and 7T dataset, while loading the data you will also resize the images on the fly,</li>\n",
    "<li>Next you will <a href='#preprocess'>preprocess</a> the loaded data: Convert the train and test lists into numpy matrices, convert the matrices type into float32, rescale the matrices using max-min strategy, reshape the arrays and finally split the data into 80% training and remaining 20% into validation set,</li>\n",
    "<li>Then, you'll create the <a href='#model'>1-Encoder-3-Decoder</a> Architecture: comprising of merge connections and multi-decoders,</li>\n",
    "<li>Next, you'll define the <a href='#loss'>loss function, define three different models and finally compile them</a>,</li>\n",
    "<li>Finally, its time that you will <a href='#train'>train</a> your merge and multi-decoder model, <a href='#test'>test</a> it on the validation data and finally compute the <a href='#results'>quantitative results</a>.  </li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Python Module Dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you start following along with this tutorial make sure you have the exact same module versions as stated below:\n",
    "\n",
    "<ul>\n",
    "    <li>Keras==2.0.4</li>\n",
    "    <li>tensorflow==1.8.0</li>\n",
    "    <li>scipy==0.19.0</li>\n",
    "    <li>numpy==1.14.5</li>\n",
    "    <li>Pillow==4.1.1</li>\n",
    "    <li>nibabel==2.1.0</li>\n",
    "    <li>scikit_learn==0.18.1</li>\n",
    "    </ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** : Before you begin, please note that the model will be trained on a system with Nvidia 1080 Ti GPU Xeon e5 GeForce processor with 32GB RAM. If you are using Jupyter Notebook, you will need to add three more lines of code where you specify CUDA device order and CUDA visible devices using a module called os.\n",
    "\n",
    "In the code below, you basically set environment variables in the notebook using os.environ. It's good to do the following before initializing Keras to limit Keras backend TensorFlow to use the first GPU. If the machine on which you train on has a GPU on 0, make sure to use 0 instead of 1. You can check that by running a simple command on your terminal: for example, nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" #model will be trained on GPU "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'modules'> </a>\n",
    "## Importing the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, you import all the required modules like tensorflow, numpy and most importantly keras and the required functions or layers like `Input`, `Conv2D`,`MaxPooling2D` etc. since you'll be using all these frameworks for training the model!<br>\n",
    "In order to read the nifti format images, you also have to import a module called `nibabel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.5/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "from keras.layers import Input,Dense,merge,Reshape,Conv2D,MaxPooling2D,UpSampling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,Sequential\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n",
    "import scipy.misc\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'data'> </a>\n",
    "## Understanding the Brain MRI 3T and 7T Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The brain MRI 3T and 7T dataset consists of 3D volumes each volume has in total 207 slices/images of brain MRI's taken at different slices of the brain. Each slice is of dimension 173 x 173. The images are single channel grayscale images. There are in total 39 subjects, each subject containing the MRI scan of a patient. The image format is not jpeg,png etc. but rather nifti format. You will see in later section how to read the nifti format images.\n",
    "\n",
    "The dataset consists of T1 modality MR images, T1 sequences are traditionally considered good for evaluation of anatomic structures. The dataset on which you will be working today consists of 3T  and 7T Brain MRI's. \n",
    "\n",
    "The dataset is public and is available for download at this <a href=\" https://www.humanconnectome.org/study/hcp-young-adult/document/1200-subjects-data-release\" target=\"_blank\">source</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28 subjects are used for training while the remaining 11 subjects will be used for testing purpose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'initialisers'> </a>\n",
    "## Defining the Initializers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first define the dimensions of the data, you will resize the image dimension from 173x173 to 176x176 in the data reading part. Here you will also define the data directory, batch size for training the model, number of channels, an `Input()` layer, train and test matrices as a list and finally for rescaling you will load the text file which has the minimum and maximum values of the MRI dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: For rescaling you can also take the maximum and minimum of the dataset since you will not have the `maxANDmin.txt` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = 173,173\n",
    "full_z = 207\n",
    "resizeTo=176\n",
    "batch_size = 32\n",
    "inChannel = outChannel = 1\n",
    "input_shape=(x,y,inChannel)\n",
    "input_img = Input(shape = (resizeTo, resizeTo, inChannel))\n",
    "inp = \"ground3T/\"\n",
    "out = \"ground7T/\"     \n",
    "train_matrix = []\n",
    "test_matrix = []\n",
    "min_max = np.loadtxt('maxANDmin.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'load'> </a>\n",
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you load the mri data using `nibabel` library and resize the images from 173 x 173 to 176 x 176 by padding zeros in x and y dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when you load a Nifti format volume, Nibabel does not load the image array. It waits until you ask for the array data. The normal way to ask for the array data is to call the get_data() method.\n",
    "\n",
    "Since you want the 2D slices instead of 3D, you will use the `train and test` lists that you initialised before; every time you read a volume, you will iterate over all the complete 207 slices of the 3D volume and append each slice one by one in to a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.listdir(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the 3T data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in folder:\n",
    "    temp = np.zeros([resizeTo,full_z,resizeTo])\n",
    "    a = nib.load(inp + f)\n",
    "    a = a.get_data()\n",
    "    temp[3:,:,3:] = a\n",
    "    a = temp\n",
    "    for j in range(full_z):\n",
    "        train_matrix.append(a[:,j,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, load the 7T data and you use the same `folder` variable for loading 7T data as well since the number of 3T and 7T MR volumes are equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in folder:\n",
    "    temp = np.zeros([resizeTo,full_z,resizeTo])\n",
    "    b = nib.load(out + f)\n",
    "    b = b.get_data()\n",
    "    temp[3:,:,3:] = b\n",
    "    b = temp\n",
    "    for j in range(full_z):\n",
    "        test_matrix.append(b[:,j,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'preprocess'> </a>\n",
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since train and test matrices is a list you will use numpy module to convert the list in to a numpy array.<br>\n",
    "<br>\n",
    "Further you will convert the `type` of the numpy array as `float32` and also rescale both the `input` and the `ground truth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_matrix = np.asarray(train_matrix)\n",
    "train_matrix = train_matrix.astype('float32')\n",
    "m = min_max[0]\n",
    "mi = min_max[1]\n",
    "train_matrix = (train_matrix - mi) / (m - mi)\n",
    "\n",
    "test_matrix = np.asarray(test_matrix)\n",
    "test_matrix = test_matrix.astype('float32')\n",
    "test_matrix = (test_matrix - mi) / (m - mi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly print the shape of train_matrix (3T) and test_matrix (7T) matrices, they should have 28 x 207 = 5796 images in total, each having a dimension of 176 x 176."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5796, 176, 176)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5796, 176, 176)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will create two new variables `augmented_images`(3T/input) and `Haugmented_images` (7T/ground truth) of the shape train and test matrix. This will be a 4D matrix in which the first dimension will be the total number of images, second and third being the dimension of each image and last dimension being the number of channels which is one in this case.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_images=np.zeros(shape=[(train_matrix.shape[0]),(train_matrix.shape[1]),(train_matrix.shape[2]),(1)])\n",
    "Haugmented_images=np.zeros(shape=[(train_matrix.shape[0]),(train_matrix.shape[1]),(train_matrix.shape[2]),(1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you will iterate over all the images one by one, each time you will reshape the train and test matrix to 176 x 176 and append it to `augmented_images`(3T/input) and `Haugmented_images` (7T/ground truth) respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(train_matrix.shape[0]):\n",
    "    augmented_images[i,:,:,0] = train_matrix[i,:,:].reshape(resizeTo,resizeTo)\n",
    "    Haugmented_images[i,:,:,0] = test_matrix[i,:,:].reshape(resizeTo,resizeTo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After all of this, it's important to partition the data. In order for your model to generalize well, you split the data into two parts: a training and a validation set. You will train your model on 80% of the data and validate it on 20% of the remaining training data.\n",
    "\n",
    "This will also help you in reducing the chances of overfitting, as you will be validating your model on data it would not have seen in training phase.\n",
    "\n",
    "You can use the train_test_split module of scikit-learn that you had defined in the begining to divide the data properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data,Label = shuffle(augmented_images,Haugmented_images, random_state=2)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, Label, test_size=0.2, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "X_test = X_test.astype('float32')\n",
    "y_test = y_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_train = X_train.astype('float32')\n",
    "y_train = y_train.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id= 'model'> </a>\n",
    "# The Model: 1-Encoder-3-Decoders!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"miccai2.jpg\"></div><br>\n",
    "<div style=\"text-align:center\">Figure: Selective Auto-Encoder Backpropogation</div><br>\n",
    "<div style=\"text-align:center\">Image taken from this  <a href=\"https://arxiv.org/abs/1806.06886\" target=\"_blank\">paper</a>.  </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you define the proposed architecture with blocks of subsequent filter layers followed by a max pooling layer in the encoder section as shown in next cell. To reconstruct the original size of image at output, an upsampling layer is introduced in each block of the decoders. While upsampling, there may be some artifacts introduced due to missing details in downsampled input of decoder. Hence, you concatenate the input of decoder with its upscaled version from the encoder in order to provide the nature of upscaled details for better reconstruction while upsampling in decoder. Indeed, adding the merge connections will yield a significant PSNR improvement (of the order of 5db). This setting of our architecture is inspired from this <a href=\" [https://arxiv.org/abs/1505.04597\" target=\"_blank\">paper</a>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The proposed approach employs a single encoder and multiple decoder architecture with a single channel input.\n",
    "Three convolutional layers are used in each block of an encoder and all the three decoders, followed by a batch normalization layer to maintain the numerical stability. \n",
    "\n",
    "The first convolutional block in the encoder has 32 filters and the number of filters doubles after each convolutional block. In all the decoders the first convolutional block has 256 filters and the number of filters are halved after each block. You will use a filter size of 3-by-3 in all convolutional blocks.\n",
    "\n",
    "Rectified Linear Unit (ReLU) will be used as an activation function in all the layers except the final layer. Since data is normalized between 0 and 1, a Sigmoid activation function is used at the final layer.\n",
    "\n",
    "\n",
    "It is well known that local image details at various scales play a significant role in image reconstruction. The proposed architecture considers images at different scales using hierarchical layers for downsampling (maxpooling) and upsampling (each for a factor of 2) in encoder and decoders, respectively. \n",
    "The encoded representation obtained after three downsampling operations brings the data from a high dimension input to a latent space representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(input_img):\n",
    "    conv1 = Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3,3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    conv1 = Conv2D(32, (3,3), activation='relu', padding='same')(conv1)\n",
    "    conv1 = BatchNormalization()(conv1)\n",
    "    pool1 = MaxPooling2D(pool_size=(2, 2))(conv1)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(pool1)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    conv2 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv2)\n",
    "    conv2 = BatchNormalization()(conv2)\n",
    "    pool2 = MaxPooling2D(pool_size=(2, 2))(conv2)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(pool2)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    conv3 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv3)\n",
    "    conv3 = BatchNormalization()(conv3)\n",
    "    pool3 = MaxPooling2D(pool_size=(2, 2))(conv3)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(pool3)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv4 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv4 = BatchNormalization()(conv4)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv4)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='relu', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    conv5 = Conv2D(512, (3, 3), activation='sigmoid', padding='same')(conv5)\n",
    "    conv5 = BatchNormalization()(conv5)\n",
    "    return conv5,conv4,conv3,conv2,conv1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_1(conv5,conv4,conv3,conv2,conv1):\n",
    "    up6 = merge([conv5, conv4], mode='concat', concat_axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up7 = UpSampling2D((2,2))(conv6)\n",
    "    up7 = merge([up7, conv3], mode='concat', concat_axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up8 = UpSampling2D((2,2))(conv7)\n",
    "    up8 = merge([up8, conv2], mode='concat', concat_axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    up9 = UpSampling2D((2,2))(conv8)\n",
    "    up9 = merge([up9, conv1], mode='concat', concat_axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    decoded_1 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(conv9)\n",
    "    return decoded_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_2(conv5,conv4,conv3,conv2,conv1):\n",
    "    up6 = merge([conv5, conv4], mode='concat', concat_axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up7 = UpSampling2D((2,2))(conv6)\n",
    "    up7 = merge([up7, conv3], mode='concat', concat_axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up8 = UpSampling2D((2,2))(conv7)\n",
    "    up8 = merge([up8, conv2], mode='concat', concat_axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    up9 = UpSampling2D((2,2))(conv8)\n",
    "    up9 = merge([up9, conv1], mode='concat', concat_axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\t\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    decoded_2 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(conv9)\n",
    "    return decoded_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder_3(conv5,conv4,conv3,conv2,conv1):\n",
    "    up6 = merge([conv5, conv4], mode='concat', concat_axis=3)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(up6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    conv6 = Conv2D(256, (3, 3), activation='relu', padding='same')(conv6)\n",
    "    conv6 = BatchNormalization()(conv6)\n",
    "    up7 = UpSampling2D((2,2))(conv6)\n",
    "    up7 = merge([up7, conv3], mode='concat', concat_axis=3)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(up7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    conv7 = Conv2D(128, (3, 3), activation='relu', padding='same')(conv7)\n",
    "    conv7 = BatchNormalization()(conv7)\n",
    "    up8 = UpSampling2D((2,2))(conv7)\n",
    "    up8 = merge([up8, conv2], mode='concat', concat_axis=3)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(up8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    conv8 = Conv2D(64, (3, 3), activation='relu', padding='same')(conv8)\n",
    "    conv8 = BatchNormalization()(conv8)\n",
    "    up9 = UpSampling2D((2,2))(conv8)\n",
    "    up9 = merge([up9, conv1], mode='concat', concat_axis=3)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(up9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\t\n",
    "    conv9 = Conv2D(32, (3, 3), activation='relu', padding='same')(conv9)\n",
    "    conv9 = BatchNormalization()(conv9)\n",
    "    decoded_3 = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(conv9)\n",
    "    return decoded_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above 4 cells you defined four functions. One for the encoder and the remaining three for decoders. Since they are functions you can define the `decoder()` function and call it three times but for better understanding you will define it three times and to avoid the issue in randomness of the three decoders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'loss'> </a>\n",
    "## Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you will employ a mean square error in which you will exclude the values(pixels) of both the ground truth `y_t` and `y_p` that are equal to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_sq_GxGy(y_t, y_p):\n",
    "    a1=1\n",
    "    where = tf.not_equal(y_t, 0)\n",
    "    a_t=tf.boolean_mask(y_t,where,name='boolean_mask')\n",
    "    a_p=tf.boolean_mask(y_p,where,name='boolean_mask')\n",
    "    return a1*(K.sqrt(K.mean((K.square(a_t-a_p)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Definition and Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you will call the `encoder` function by passing in the input to it. Since you are using merge connection in your architecture the `encoder` function will return output of five `convolution` layers which you then `merge` with all three decoders output separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv5,conv4,conv3,conv2,conv1 = encoder(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:2: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  \n",
      "/usr/local/lib/python3.5/dist-packages/keras/legacy/layers.py:460: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:10: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:18: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.5/dist-packages/ipykernel_launcher.py:26: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:1257: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "autoencoder_1 = Model(input_img, decoder_1(conv5,conv4,conv3,conv2,conv1))\n",
    "autoencoder_1.compile(loss=root_mean_sq_GxGy, optimizer = RMSprop())\n",
    "\n",
    "autoencoder_2 = Model(input_img, decoder_2(conv5,conv4,conv3,conv2,conv1))\n",
    "autoencoder_2.compile(loss=root_mean_sq_GxGy, optimizer = RMSprop())\n",
    "\n",
    "autoencoder_3 = Model(input_img, decoder_3(conv5,conv4,conv3,conv2,conv1))\n",
    "autoencoder_3.compile(loss=root_mean_sq_GxGy, optimizer = RMSprop())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'train'> </a>\n",
    "## Train the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will save the weights only when the `peak signal-to-noise ratio` on the validation data improves. So for that you will define a `psnr_gray_channel` list in which you will append a default value as 1, you will append 1 since you need to initialise the list with a random number and the PSNR will be far more than this value even in initial stages of training and this will act as a mere dummy number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You wil initalise the learning rate to be 1e-3 and use a learning rate decay strategy where you will decrease your learning rate by 10% from its current value after every 20 epochs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "psnr_gray_channel = []\n",
    "psnr_gray_channel.append(1)\n",
    "learning_rate = 0.001\n",
    "j=0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: The next complete code needs to be run in one complete cell but for better understanding of the complete training process it will be divided into small cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained for 500 epochs, the initial learning rate is 1e-3 as defined in the previous cell. You will also save the `PSNR`, `MSE` values after every epoch of 7T with predicted. You will use `K.set_value` function to change the learning rate of all three models by 10% but after every 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for jj in range(500):\n",
    "    myfile_valid_psnr_7T = open('../1_encoder_3_decoders_complete_slices_single_channel/validation_psnr7T_1encoder_3decoders.txt', 'a')\n",
    "    myfile_valid_mse_7T = open('../1_encoder_3_decoders_complete_slices_single_channel/validation_mse7T_1encoder_3decoders.txt', 'a')\n",
    "  \n",
    "    K.set_value(autoencoder_1.optimizer.lr, learning_rate)\n",
    "    K.set_value(autoencoder_2.optimizer.lr, learning_rate)\n",
    "    K.set_value(autoencoder_3.optimizer.lr, learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you shuffle the input 3T and ground truth 7T images to avoid the network from overfitting since not shuffling will enforce the model to see the samples in same order after every epoch. Then, you will calculate the number of batches that will be created based on the `batch_size` that you had defined before. Finally, you start iterating over the `num_batches`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X,train_Y = shuffle(X_train,y_train)\n",
    "print (\"Epoch is: %d\\n\" % j)\n",
    "print (\"Number of batches: %d\\n\" % int(len(train_X)/batch_size))\n",
    "num_batches = int(len(train_X)/batch_size)\n",
    "for batch in range(num_batches):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apart from storing PSNR values you will also store the losses of all three autoencoders and the three decoders respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile_ae1_loss = open('../1_encoder_3_decoders_complete_slices_single_channel/ae1_train_loss_1encoder_3decoders.txt', 'a')\n",
    "myfile_ae2_loss = open('../1_encoder_3_decoders_complete_slices_single_channel/ae2_train_loss_1encoder_3decoders.txt', 'a')\n",
    "myfile_ae3_loss = open('../1_encoder_3_decoders_complete_slices_single_channel/ae3_train_loss_1encoder_3decoders.txt', 'a')\n",
    "myfile_dec1_loss = open('../1_encoder_3_decoders_complete_slices_single_channel/dec1_train_loss_1encoder_3decoders.txt', 'a')\n",
    "myfile_dec2_loss = open('../1_encoder_3_decoders_complete_slices_single_channel/dec2_train_loss_1encoder_3decoders.txt', 'a')\n",
    "myfile_dec3_loss = open('../1_encoder_3_decoders_complete_slices_single_channel/dec3_train_loss_1encoder_3decoders.txt', 'a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in every batch ypu want the next 32 (batch_size) samples to be seen by the model, the next cell does that for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_train_X = train_X[batch*batch_size:min((batch+1)*batch_size,len(train_X)),:]\n",
    "batch_train_Y = train_Y[batch*batch_size:min((batch+1)*batch_size,len(train_Y)),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next for our Minimum autoencoder strategy to work, after every epoch you test on the training data using Keras `test_on_batch` function that will give you three losses respectively and finally you print them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_1 = autoencoder_1.test_on_batch(batch_train_X,batch_train_Y)\n",
    "loss_2 = autoencoder_2.test_on_batch(batch_train_X,batch_train_Y)\n",
    "loss_3 = autoencoder_3.test_on_batch(batch_train_X,batch_train_Y)\n",
    "print ('epoch_num: %d batch_num: %d Test_loss_1: %f\\n' % (j,batch,loss_1))\n",
    "print ('epoch_num: %d batch_num: %d Test_loss_2: %f\\n' % (j,batch,loss_2))\n",
    "print ('epoch_num: %d batch_num: %d Test_loss_3: %f\\n' % (j,batch,loss_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There can be six possible conditions in your network:<br>\n",
    "loss_1: Autoencoder 1 loss<br>\n",
    "loss_2: Autoencoder 2 loss<br>\n",
    "loss_3: Autoencoder 3 loss<br>\n",
    "<ul> \n",
    "<li> loss_1 can be greater than loss_2 and loss_3, if this is true then you train using only Autoencoder 1. You will make Autoencoder 2 and Autoencoder 3 Encoder part's as False and train only their decoders and finally you will write all the losses in the text files.</li><br>\n",
    "\n",
    "<li> loss_2 can be greater than loss_1 and loss_3, if this is true then you train using only Autoencoder 2. You make Autoencoder 1 and Autoencoder 3 Encoder part's as False and train only their decoders and finally you will write all the losses in the text files.</li><br>\n",
    "\n",
    "<li> loss_3 can be greater than loss_1 and loss_2, if this is true then you train using only Autoencoder 3. You make Autoencoder 1 and Autoencoder 2 Encoder part's as False and train only their decoders and finally you will write all the losses in the text files.</li><br>\n",
    "\n",
    "<li> loss_1 can be equal to loss_2, if this is true then you train using either Autoencoder 1 or 2. You make Autoencoder 3  Encoder part's as False along with the Autoencoder you didn't chose between 1 and 2, and train only their decoders and finally you will write all the losses in the text files.</li><br>\n",
    "\n",
    "<li> loss_2 can be equal to loss_3, if this is true then you train using either Autoencoder 2 or 3. You make Autoencoder 1  Encoder part's as False along with the Autoencoder you didn't chose between 2 and 3, and train only their decoders and finally you will write all the losses in the text files.</li><br>\n",
    "\n",
    "\n",
    "<li> loss_3 can be equal to loss_1, if this is true then you train using either Autoencoder 3 or 1. You make Autoencoder 2  Encoder part's as False along with the Autoencoder you didn't chose between 3 and 1, and train only their decoders and finally you will write all the losses in the text files.</li><br>\n",
    "\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align:center\"><img src=\"miccai1.jpg\"></div><br>\n",
    "<div style=\"text-align:center\">Figure: Architecture of the Model</div><br>\n",
    "<div style=\"text-align:center\">Image taken from this  <a href=\"https://arxiv.org/abs/1806.06886\" target=\"_blank\">paper</a>.  </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if loss_1 < loss_2 and loss_1 < loss_3:\n",
    "    train_1 = autoencoder_1.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_ae1_loss.write(\"%f \\n\" % (train_1))\n",
    "    print ('epoch_num: %d batch_num: %d AE_Train_loss_1: %f\\n' % (j,batch,train_1))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d AE_Train_loss_1: %f\\n' % (j,batch,train_1))\n",
    "    for layer in autoencoder_2.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    for layer in autoencoder_3.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    #autoencoder_2.summary()\n",
    "    #autoencoder_3.summary()\n",
    "    train_2 = autoencoder_2.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    train_3 = autoencoder_3.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_dec2_loss.write(\"%f \\n\" % (train_2))\n",
    "    myfile_dec3_loss.write(\"%f \\n\" % (train_3))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_2: %f\\n' % (j,batch,train_2))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_loss_3: %f\\n' % (j,batch,train_3))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_2: %f\\n' % (j,batch,train_2))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_loss_3: %f\\n' % (j,batch,train_3))\n",
    "elif loss_2 < loss_1 and loss_2 < loss_3:\n",
    "    train_2 = autoencoder_2.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_ae2_loss.write(\"%f \\n\" % (train_2))\n",
    "    print ('epoch_num: %d batch_num: %d AE_Train_loss_2: %f\\n' % (j,batch,train_2))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d AE_Train_loss_2: %f\\n' % (j,batch,train_2))\n",
    "    for layer in autoencoder_1.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    for layer in autoencoder_3.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    #autoencoder_1.summary()\n",
    "    #autoencoder_3.summary()\n",
    "    train_1 = autoencoder_1.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    train_3 = autoencoder_3.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_dec1_loss.write(\"%f \\n\" % (train_1))\n",
    "    myfile_dec3_loss.write(\"%f \\n\" % (train_3))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_1: %f\\n' % (j,batch,train_1))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_3: %f\\n' % (j,batch,train_3))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_1: %f\\n' % (j,batch,train_1))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_3: %f\\n' % (j,batch,train_3))\n",
    "elif loss_3 < loss_1 and loss_3 < loss_2:\n",
    "    train_3 = autoencoder_3.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_ae3_loss.write(\"%f \\n\" % (train_3))\n",
    "    print ('epoch_num: %d batch_num: %d AE_Train_loss_3: %f\\n' % (j,batch,train_3))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d AE_Train_loss_3: %f\\n' % (j,batch,train_3))\n",
    "    for layer in autoencoder_1.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    for layer in autoencoder_2.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    #autoencoder_1.summary()\n",
    "    #autoencoder_2.summary()\n",
    "    train_1 = autoencoder_1.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    train_2 = autoencoder_2.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_dec1_loss.write(\"%f \\n\" %(train_1))\n",
    "    myfile_dec2_loss.write(\"%f \\n\" % (train_2))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_1: %f\\n' % (j,batch,train_1))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_2: %f\\n' % (j,batch,train_2))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_1: %f\\n' % (j,batch,train_1))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_2: %f\\n' % (j,batch,train_2))\n",
    "elif loss_1 == loss_2:\n",
    "    train_1 = autoencoder_1.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_ae1_loss.write(\"%f \\n\" % (train_1))\n",
    "    print ('epoch_num: %d batch_num: %d AE_Train_loss_1_equal_state: %f\\n' % (j,batch,train_1))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d AE_Train_loss_1_equal_state: %f\\n' % (j,batch,train_1))\n",
    "    for layer in autoencoder_3.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    for layer in autoencoder_2.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    #autoencoder_2.summary()\n",
    "    #autoencoder_3.summary()\n",
    "    train_2 = autoencoder_2.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    train_3 = autoencoder_3.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_dec2_loss.write(\"%f \\n\" % (train_2))\n",
    "    myfile_dec3_loss.write(\"%f \\n\" % (train_3))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_2_equal_state: %f\\n' % (j,batch,train_2))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_3_equal_state: %f\\n' % (j,batch,train_3))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_2_equal_state: %f\\n' % (j,batch,train_2))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_3_equal_state: %f\\n' % (j,batch,train_3))\n",
    "elif loss_2 == loss_3: \n",
    "    train_2 = autoencoder_2.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_ae2_loss.write(\"%f \\n\" % (train_2))\n",
    "    print ('epoch_num: %d batch_num: %d AE_Train_loss_2_equal_state: %f\\n' % (j,batch,train_2))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d AE_Train_loss_2_equal_state: %f\\n' % (j,batch,train_2))\n",
    "    for layer in autoencoder_1.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    for layer in autoencoder_3.layers[:34]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    #autoencoder_2.summary()\n",
    "    #autoencoder_3.summary()\n",
    "    train_1 = autoencoder_1.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    train_3 = autoencoder_3.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_dec1_loss.write(\"%f \\n\" % (train_1))\n",
    "    myfile_dec3_loss.write(\"%f \\n\" % (train_3))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_1_equal_state: %f\\n' % (j,batch,train_1))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_3_equal_state: %f\\n' % (j,batch,train_3))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_1_equal_state: %f\\n' % (j,batch,train_1))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_3_equal_state: %f\\n' % (j,batch,train_3))\n",
    "elif loss_3 == loss_1:\n",
    "    train_1 = autoencoder_1.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_ae1_loss.write(\"%f \\n\" % (train_1))\n",
    "    print ('epoch_num: %d batch_num: %d AE_Train_loss_1_equal_state: %f\\n' % (j,batch,train_1))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d AE_Train_loss_1_equal_state: %f\\n' % (j,batch,train_1))\n",
    "    for layer in autoencoder_2.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    for layer in autoencoder_3.layers[:34]:\n",
    "        layer.trainable = False\n",
    "    #autoencoder_2.summary()\n",
    "    #autoencoder_3.summary()\n",
    "    train_2 = autoencoder_2.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    train_3 = autoencoder_3.train_on_batch(batch_train_X,batch_train_Y)\n",
    "    myfile_dec2_loss.write(\"%f \\n\" % (train_2))\n",
    "    myfile_dec3_loss.write(\"%f \\n\" % (train_3))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_2_equal_state: %f\\n' % (j,batch,train_2))\n",
    "    print ('epoch_num: %d batch_num: %d Decoder_Train_loss_3_equal_state: %f\\n' % (j,batch,train_3))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_2_equal_state: %f\\n' % (j,batch,train_2))\n",
    "    #myfile.write('epoch_num: %d batch_num: %d Decoder_Train_loss_3_equal_state: %f\\n' % (j,batch,train_3))\n",
    "\n",
    "\n",
    "    myfile_ae1_loss.close()\n",
    "    myfile_ae2_loss.close()\n",
    "    myfile_ae3_loss.close()\n",
    "    myfile_dec1_loss.close()\n",
    "    myfile_dec2_loss.close()\n",
    "    myfile_dec3_loss.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an important step, since you made few layers in the above conditions False, you need to make those layers True, so that all the autoencoder layers can be used for the `test_on_batch` function and you don't want that they are kept False throughout the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in autoencoder_1.layers[:34]:\n",
    "            layer.trainable = True\n",
    "for layer in autoencoder_2.layers[:34]:\n",
    "            layer.trainable = True\n",
    "for layer in autoencoder_3.layers[:34]:\n",
    "            layer.trainable = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will save weights in three different ways, below are two of those ways. One you will save weights after every 100 epochs and second you will save weights after every epoch basically they are overwritten after every epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if jj % 100 ==0:\n",
    "            autoencoder_1.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE1_\" + str(jj)+\".h5\")\n",
    "            autoencoder_2.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE2_\" + str(jj)+\".h5\")\n",
    "            autoencoder_3.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE3_\" + str(jj)+\".h5\")\n",
    "\n",
    "\n",
    "autoencoder_1.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE1.h5\")\n",
    "autoencoder_2.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE2.h5\")\n",
    "autoencoder_3.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE3.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'test'> </a>\n",
    "## Testing on Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will shuffle the validation data first and then use the same 6 possible conditions that may happen as defined above. So which ever condition is True you will `test` your model using that particular autoencoder.<br>\n",
    "<br>\n",
    "Then you calculate two metrics `MSE` and `PSNR` between decoded_imgs(predicted) and ground truth, finally you save them in the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,y_test = shuffle(X_test,y_test)\n",
    "if loss_1 < loss_2 and loss_1 < loss_3:\n",
    "    decoded_imgs = autoencoder_1.predict(X_test)\n",
    "    mse_7T=  np.mean((y_test[:,:,:,0] - decoded_imgs[:,:,:,0]) ** 2)\n",
    "    check_7T = math.sqrt(mse_7T)\n",
    "    psnr_7T = 20 * math.log10( 1.0 / check_7T)\n",
    "\n",
    "\n",
    "    myfile_valid_psnr_7T.write(\"%f \\n\" % (psnr_7T))\n",
    "    myfile_valid_mse_7T.write(\"%f \\n\" % (mse_7T))\n",
    "\n",
    "    #print (check)\n",
    "elif loss_2 < loss_1 and loss_2 < loss_3:\n",
    "    decoded_imgs = autoencoder_2.predict(X_test)\n",
    "    mse_7T=  np.mean((y_test[:,:,:,0] - decoded_imgs[:,:,:,0]) ** 2)\n",
    "    check_7T = math.sqrt(mse_7T)\n",
    "    psnr_7T = 20 * math.log10( 1.0 / check_7T)\n",
    "    \n",
    "    myfile_valid_psnr_7T.write(\"%f \\n\" % (psnr_7T))\n",
    "    myfile_valid_mse_7T.write(\"%f \\n\" % (mse_7T))\n",
    "\n",
    "    #print (check)\n",
    "elif loss_3 < loss_2 and loss_3 < loss_1:\n",
    "    decoded_imgs = autoencoder_3.predict(X_test)\n",
    "    mse_7T=  np.mean((y_test[:,:,:,0] - decoded_imgs[:,:,:,0]) ** 2)\n",
    "    check_7T = math.sqrt(mse_7T)\n",
    "    psnr_7T = 20 * math.log10( 1.0 / check_7T)\n",
    "\n",
    "    myfile_valid_psnr_7T.write(\"%f \\n\" % (psnr_7T))\n",
    "    myfile_valid_mse_7T.write(\"%f \\n\" % (mse_7T))\n",
    "    #print (check)\n",
    "    \n",
    "elif loss_1 == loss_2:\n",
    "    decoded_imgs = autoencoder_1.predict(X_test)\n",
    "    mse_7T=  np.mean((y_test[:,:,:,0] - decoded_imgs[:,:,:,0]) ** 2)\n",
    "    check_7T = math.sqrt(mse_7T)\n",
    "    psnr_7T = 20 * math.log10( 1.0 / check_7T)\n",
    "\n",
    "    myfile_valid_psnr_7T.write(\"%f \\n\" % (psnr_7T))\n",
    "    myfile_valid_mse_7T.write(\"%f \\n\" % (mse_7T))\n",
    "\n",
    "\n",
    "elif loss_2 == loss_3:\n",
    "    decoded_imgs = autoencoder_2.predict(X_test)\n",
    "    mse_7T=  np.mean((y_test[:,:,:,0] - decoded_imgs[:,:,:,0]) ** 2)\n",
    "    check_7T = math.sqrt(mse_7T)\n",
    "    psnr_7T = 20 * math.log10( 1.0 / check_7T)\n",
    "\n",
    "    myfile_valid_psnr_7T.write(\"%f \\n\" % (psnr_7T))\n",
    "    myfile_valid_mse_7T.write(\"%f \\n\" % (mse_7T))\n",
    "\n",
    "\n",
    "elif loss_3 == loss_1:\n",
    "    decoded_imgs = autoencoder_3.predict(X_test)\n",
    "    mse_7T=  np.mean((y_test[:,:,:,0] - decoded_imgs[:,:,:,0]) ** 2)\n",
    "    check_7T = math.sqrt(mse_7T)\n",
    "    psnr_7T = 20 * math.log10( 1.0 / check_7T)\n",
    "\n",
    "    myfile_valid_psnr_7T.write(\"%f \\n\" % (psnr_7T))\n",
    "    myfile_valid_mse_7T.write(\"%f \\n\" % (mse_7T))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you save the weights only when the `PSNR` between the predicted 7T and ground truth (7T) is the maximum compared to the previous `PSNR` values stored in the `psnr_gray_channel` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max(psnr_gray_channel) < psnr_7T:\n",
    "            autoencoder_1.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE1_\" + str(jj)+\".h5\")\n",
    "            autoencoder_2.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE2_\" + str(jj)+\".h5\")\n",
    "            autoencoder_3.save_weights(\"../Model/CROSSVAL1/CROSSVAL1_AE3_\" + str(jj)+\".h5\")\n",
    "\n",
    "    psnr_gray_channel.append(psnr_7T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id= 'results'> </a>\n",
    "## Saving Input, Ground Truth and Decoded: Quantitative Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will define a numpy matrix `temp` that has a size of 176 x 528, since there will be 3 images saved in one row each with a dimension of 176 x 176. You will save any one of the image out of all the validation 3T, 7T and predicted and  multiply the matrix by 255 since the images were scaled down between 0 and 1.<br>\n",
    "<br>\n",
    "Finally using `scipy` function you will save the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.zeros([resizeTo,resizeTo*3])\n",
    "temp[:resizeTo,:resizeTo] = X_test[0,:,:,0]\n",
    "temp[:resizeTo,resizeTo:resizeTo*2] = y_test[0,:,:,0]\n",
    "temp[:resizeTo,2*resizeTo:] = decoded_imgs[0,:,:,0]\n",
    "temp = temp*255\n",
    "scipy.misc.imsave('../Results/1_encoder_3_decoders_complete_slices_single_channel/' + str(j) + '.jpg', temp)\n",
    "j +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's close the `PSNR` and `MSE` files at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfile_valid_psnr_7T.close()\n",
    "myfile_valid_mse_7T.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, at the end you will reduce the learning rate by 10% of its current value after every 20 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if jj % 20 ==0:\n",
    "        learning_rate = learning_rate - learning_rate * 0.10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
